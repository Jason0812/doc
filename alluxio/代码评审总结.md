# Alluxio 代码评审总结

下面是对本次设计方案评审的总结；

## Client对MountTable进行缓存的讨论

1. 当server端进行umount操作的时候；client端没有办法知悉；

在使用时进行限制；尽量不要使用umount操作；

2. 进行mount之后，对正在运行的client不会更新；

从对业务场景进行分析，新mount的信息，不会影响正在执行的任务；

**结论**：鉴于业务的使用场景和实现的复杂度问题，采用目前的client端cache MountTable的做法；



## Alluxio shell 命令的支持

应当调用Alluxio shell和HDFS shell呈现出一样的结果；

<font color = red> 应当支持，后期回去实现</font>



## Load data的时候数据的一致性检测

1. 对已存在的路径，进行load操作的时候，应当判别文件的一致性；这里的一致性包括：
   1. 是否新加了文件和文件夹；如果新加了，在open的时候，应当会去load；<font color = red> 应当支持，可以实现</font>；
   2. 删除文件，然后重新添加重名的文件；这种情况当删除和创建都是从PRoxy走的话，应该不会出现文件的不一致性；在create的时候，应当根据OverWrite决定是否去Alluxio Space中删除；<font color = red>在create文件中添加这个逻辑, 已经实现</font>
   3. 当使用不同的入口去delete和create同名文件的时候，这个时候的检测机制；<font color = red>有待研究</font>；
   4. 文件个数保持不变，但文件内容变了之后，去check，然后去重新加载；<font color = red>有待研究</font>；

## HDFS集群的配置

当前HDFS集群的配置是放在Client端的；这样后期添加集群的时候，需要更新client端的`hdfs-site.xml`，添加新加集群的nameservice和nn1、nn2的hostname信息；

1. 运维起来较麻烦；
2. 可以放在master端，在client去getMountPoint的时候将conf信息一并拿回来；这样只需要在master端配置；

**结论**：鉴于添加集群的操作不是很频繁，每次从master端get更多的信息，会带来性能损耗；所以维持当前的做法，在client进行配置；<font color = red> 后期可以实现从Master端获取conf信息，看性能损耗多大</font>

## rename的问题

当前的rename不支持跨scheme的rename，同时也不支持从UserMustCacheList rename到persist 目录；

1. 对于hive的中间结果；hive的结果是先写tmp目录，然后rename到persist 目录的；<font color = red>当前不支持，是否支持应当参考hive的数据流</font>
2. Hbase会用到不同scheme的rename；比如说将user目录下的Hbase目录汇聚到hbase的表格目录中，由于不同的user会放大不同的集群，所以会存在跨集群的rename操作；<font color = red> 当前不支持，梳理Hbase的需求，决定是否要支持这个操作</font>
3. CBT目录，源目录和目标目录不在一个集群，rename会出现跨集群的问题，同时也会出现跨集群的读取；<font color = red>指定cbt的目录在用户的目录之下去解决这个问题</font>

## Flume没有用到Append机制

需要调研Append方法在什么情况下会使用；

机器学习写日志的情况下，会调用append的方法去写日志（应用日志）；

## UserMustCacheList应该放在client端还是master端

<font color = red>实现将UserMustCacheList放在Master端， 然后client端去刷</font>

## read多副本的问题

在测试完之后，将想法和结论和社区进行沟通，看看有没有更好的实现方法；

## TODO

1. 实现方案评审中需要实现的功能；
2. Hive的数据流的分析；
3. Hbase的数据流分析；
4. spark的数据流分析；

<font color = read> 研究文件如何自动更新</font>